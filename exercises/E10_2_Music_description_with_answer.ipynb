{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "neeKsTWHEhMt"
      },
      "source": [
        "## Exercise 10-2: Music description with Essentia\n",
        "\n",
        "In this exercise, you will extend the sound clustering task you did in E9 to a larger set of instrument classes and explore possible improvements to it. By doing this exercise, you will get hands on experience with Essentia and better insights into complexities arising in a real world Music Information Retrieval problem.\n",
        "\n",
        "In E9, you explored the tasks of clustering with sound excerpts of three instruments, three classes. As we increse the number of sounds and of classes, the average performance degrades. In such situations, clustering performance can be improved by better selecting the descriptors or by improving the actual computation of the descriptors.\n",
        "\n",
        "You need to install Essentia to compute some of the descriptors that you will be exploring for the task. You can find the download and install instructions for Essentia here: http://essentia.upf.edu/. Essentia has extensive documentation that will be useful in this assignment http://essentia.upf.edu/documentation/index.html.\n",
        "\n",
        "If you do not want, or can, install Essentia, you can use a docker image to run a jupyter notebook server with Essentia included in it. You need to first install docker, https://www.docker.com/, and then run, in the Terminal, `docker-compose up` from the root directory of sms-tools which will use the file `docker-compose.yml` to call the image appropiately."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#if want to run this notebook in google colab you should uncomment the following commands\n",
        "!pip install sms-tools\n",
        "!git clone https://github.com/MTG/sms-tools-materials.git\n",
        "!pip install numpy==1.23.5\n",
        "!pip install git+https://github.com/mtg/freesound-python.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ppi8dfgsMbzZ",
        "outputId": "c30b67d3-eed9-4b4d-a737-81088afa2d6e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sms-tools in /usr/local/lib/python3.11/dist-packages (1.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from sms-tools) (3.10.0)\n",
            "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.11/dist-packages (from sms-tools) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sms-tools) (1.15.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->sms-tools) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->sms-tools) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->sms-tools) (4.58.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->sms-tools) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->sms-tools) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->sms-tools) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->sms-tools) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->sms-tools) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->sms-tools) (1.17.0)\n",
            "fatal: destination path 'sms-tools-materials' already exists and is not an empty directory.\n",
            "Requirement already satisfied: numpy==1.23.5 in /usr/local/lib/python3.11/dist-packages (1.23.5)\n",
            "Collecting git+https://github.com/mtg/freesound-python.git\n",
            "  Cloning https://github.com/mtg/freesound-python.git to /tmp/pip-req-build-cqcrof3m\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/mtg/freesound-python.git /tmp/pip-req-build-cqcrof3m\n",
            "  Resolved https://github.com/mtg/freesound-python.git to commit 5be99a3689d17303c01cb122bbb0d5a96eba04f6\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests<3.0,>2.27 in /usr/local/lib/python3.11/dist-packages (from freesound-python==1.1) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>2.27->freesound-python==1.1) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>2.27->freesound-python==1.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>2.27->freesound-python==1.1) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>2.27->freesound-python==1.1) (2025.4.26)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsUd6SXWEhMu"
      },
      "source": [
        "## Part 1: Download sounds\n",
        "\n",
        "Choose at least 10 different instrumental sounds classes from the following possible classes: violin, guitar, bassoon, trumpet, clarinet, cello, naobo, snare drum, flute, mridangam, daluo, xiaoluo. For each instrument class, use `download_sounds_freesound()` to download the audio and descriptors of 20 examples of representative single notes/strokes of each instrument. Since you will use the sounds also to extract descriptors using Essentia, we will use high quality mp3. We use the call `fs.FSRequest.retrieve(sound.previews.preview_hq_mp3, fsClnt, mp3Path)` within `download_sounds_freesound()`.\n",
        "\n",
        "Explain your choices, query texts, and the tags you used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "62gTgXpzEhMu"
      },
      "outputs": [],
      "source": [
        "import os, sys\n",
        "import numpy as np\n",
        "import json\n",
        "import freesound as fs\n",
        "from scipy.cluster.vq import vq, kmeans, whiten\n",
        "\n",
        "descriptors = [ 'lowlevel.spectral_centroid.mean',\n",
        "                'lowlevel.spectral_contrast.mean',\n",
        "                'lowlevel.dissonance.mean',\n",
        "                'lowlevel.hfc.mean',\n",
        "                'lowlevel.mfcc.mean',\n",
        "                'sfx.logattacktime.mean',\n",
        "                'sfx.inharmonicity.mean']\n",
        "\n",
        "# Mapping of descriptors\n",
        "descriptorMapping = { 0: 'lowlevel.spectral_centroid.mean',\n",
        "                      1: 'lowlevel.dissonance.mean',\n",
        "                      2: 'lowlevel.hfc.mean',\n",
        "                      3: 'sfx.logattacktime.mean',\n",
        "                      4: 'sfx.inharmonicity.mean',\n",
        "                      5: 'lowlevel.spectral_contrast.mean.0',\n",
        "                      6: 'lowlevel.spectral_contrast.mean.1',\n",
        "                      7: 'lowlevel.spectral_contrast.mean.2',\n",
        "                      8: 'lowlevel.spectral_contrast.mean.3',\n",
        "                      9: 'lowlevel.spectral_contrast.mean.4',\n",
        "                      10: 'lowlevel.spectral_contrast.mean.5',\n",
        "                      11: 'lowlevel.mfcc.mean.0',\n",
        "                      12: 'lowlevel.mfcc.mean.1',\n",
        "                      13: 'lowlevel.mfcc.mean.2',\n",
        "                      14: 'lowlevel.mfcc.mean.3',\n",
        "                      15: 'lowlevel.mfcc.mean.4',\n",
        "                      16: 'lowlevel.mfcc.mean.5'\n",
        "                    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Z70N6wPsEhMv"
      },
      "outputs": [],
      "source": [
        "def download_sounds_freesound(queryText = \"\", tag=None, duration=None, API_Key = \"\", outputDir = \"\", topNResults = 5, featureExt = '.json'):\n",
        "  \"\"\"\n",
        "  This function downloads sounds and their descriptors from freesound using the queryText and the\n",
        "  tag specified in the input. Additionally, you can also specify the duration range to filter sounds\n",
        "  based on duration.\n",
        "\n",
        "  Inputs:\n",
        "        (Input parameters marked with a * are optional)\n",
        "        queryText (string): query text for the sounds (eg. \"violin\", \"trumpet\", \"cello\", \"bassoon\" etc.)\n",
        "        tag* (string): tag to be used for filtering the searched sounds. (eg. \"multisample\",\n",
        "                       \"single-note\" etc.)\n",
        "        duration* (tuple): min and the max duration (seconds) of the sound to filter, eg. (0.2,15)\n",
        "        API_Key (string): your api key, which you can obtain from : www.freesound.org/apiv2/apply/\n",
        "        outputDir (string): path to the directory where you want to store the sounds and their\n",
        "                            descriptors\n",
        "        topNResults (integer): number of results(sounds) that you want to download\n",
        "        featureExt (string): file extension for storing sound descriptors\n",
        "  output:\n",
        "        This function downloads sounds and descriptors, and then stores them in outputDir. In\n",
        "        outputDir it creates a directory of the same name as that of the queryText. In this\n",
        "        directory outputDir/queryText it creates a directory for every sound with the name\n",
        "        of the directory as the sound id. Additionally, this function also dumps a text file\n",
        "        containing sound-ids and freesound links for all the downloaded sounds in the outputDir.\n",
        "        NOTE: If the directory outputDir/queryText exists, it deletes the existing contents\n",
        "        and stores only the sounds from the current query.\n",
        "  \"\"\"\n",
        "\n",
        "  # Checking for the compulsory input parameters\n",
        "  if queryText == \"\":\n",
        "    print(\"\\n\")\n",
        "    print(\"Provide a query text to search for sounds\")\n",
        "    return -1\n",
        "\n",
        "  if API_Key == \"\":\n",
        "    print(\"\\n\")\n",
        "    print(\"You need a valid freesound API key to be able to download sounds.\")\n",
        "    print(\"Please apply for one here: www.freesound.org/apiv2/apply/\")\n",
        "    print(\"\\n\")\n",
        "    return -1\n",
        "\n",
        "  if outputDir == \"\" or not os.path.exists(outputDir):\n",
        "    print(\"\\n\")\n",
        "    print(\"Please provide a valid output directory. This will be the root directory for storing sounds and descriptors\")\n",
        "    return -1\n",
        "\n",
        "  # Setting up the Freesound client and the authentication key\n",
        "  fsClnt = fs.FreesoundClient()\n",
        "  fsClnt.set_token(API_Key,\"token\")\n",
        "\n",
        "  # Creating a duration filter string that the Freesound API understands\n",
        "  if duration and type(duration) == tuple:\n",
        "    flt_dur = \" duration:[\" + str(duration[0])+ \" TO \" +str(duration[1]) + \"]\"\n",
        "  else:\n",
        "    flt_dur = \"\"\n",
        "\n",
        "  if tag and type(tag) == str:\n",
        "    flt_tag = \"tag:\"+tag\n",
        "  else:\n",
        "    flt_tag = \"\"\n",
        "\n",
        "  # Querying Freesound\n",
        "  page_size = 30\n",
        "  if not flt_tag + flt_dur == \"\":\n",
        "    qRes = fsClnt.text_search(query=queryText ,filter = flt_tag + flt_dur,sort=\"score\", fields=\"id,name,previews,username,url,analysis\", descriptors=','.join(descriptors), page_size=page_size, normalized=1)\n",
        "  else:\n",
        "    qRes = fsClnt.text_search(query=queryText ,sort=\"score\",fields=\"id,name,previews,username,url,analysis\", descriptors=','.join(descriptors), page_size=page_size, normalized=1)\n",
        "\n",
        "  outDir2 = os.path.join(outputDir, queryText)\n",
        "  if os.path.exists(outDir2):             # If the directory exists, it deletes it and starts fresh\n",
        "      os.system(\"rm -r \" + outDir2)\n",
        "  os.mkdir(outDir2)\n",
        "\n",
        "  pageNo = 1\n",
        "  sndCnt = 0\n",
        "  indCnt = 0\n",
        "  totalSnds = min(qRes.count,200)   # System quits after trying to download after 200 times\n",
        "\n",
        "  # Creating directories to store output and downloading sounds and their descriptors\n",
        "  downloadedSounds = []\n",
        "  while(1):\n",
        "    if indCnt >= totalSnds:\n",
        "      print(\"Not able to download required number of sounds. Either there are not enough search results on freesound for your search query and filtering constraints or something is wrong with this script.\")\n",
        "      break\n",
        "    sound = qRes[indCnt - ((pageNo-1)*page_size)]\n",
        "    print(\"Downloading mp3 preview and descriptors for sound with id: %s\"%str(sound.id))\n",
        "    outDir1 = os.path.join(outputDir, queryText, str(sound.id))\n",
        "    if os.path.exists(outDir1):\n",
        "      os.system(\"rm -r \" + outDir1)\n",
        "    os.system(\"mkdir \" + outDir1)\n",
        "\n",
        "    mp3Path = os.path.join(outDir1,  str(sound.previews.preview_lq_mp3.split(\"/\")[-1]))\n",
        "    ftrPath = mp3Path.replace('.mp3', featureExt)\n",
        "\n",
        "    try:\n",
        "\n",
        "      fs.FSRequest.retrieve(sound.previews.preview_hq_mp3, fsClnt, mp3Path)\n",
        "      # Initialize a dictionary to store descriptors\n",
        "      features = {}\n",
        "      # Obtaining all the descriptors\n",
        "      for desc in descriptors:\n",
        "        features[desc]=[]\n",
        "        features[desc].append(eval(\"sound.analysis.\"+desc))\n",
        "\n",
        "      # Once we have all the descriptors, store them in a json file\n",
        "      json.dump(features, open(ftrPath,'w'))\n",
        "      sndCnt+=1\n",
        "      downloadedSounds.append([str(sound.id), sound.url])\n",
        "\n",
        "    except:\n",
        "      if os.path.exists(outDir1):\n",
        "        os.system(\"rm -r \" + outDir1)\n",
        "\n",
        "    indCnt +=1\n",
        "\n",
        "    if indCnt%page_size==0:\n",
        "      qRes = qRes.next_page()\n",
        "      pageNo+=1\n",
        "\n",
        "    if sndCnt>=topNResults:\n",
        "      break\n",
        "  # Dump the list of files and Freesound links\n",
        "  fid = open(os.path.join(outDir2, queryText+'_SoundList.txt'), 'w')\n",
        "  for elem in downloadedSounds:\n",
        "    fid.write('\\t'.join(elem)+'\\n')\n",
        "  fid.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "id": "gX9teNjJEhMv",
        "outputId": "10921191-1bca-44ad-f710-dd906177f28d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading mp3 preview and descriptors for sound with id: 277127\n",
            "Downloading mp3 preview and descriptors for sound with id: 277126\n",
            "Downloading mp3 preview and descriptors for sound with id: 277125\n",
            "Downloading mp3 preview and descriptors for sound with id: 277124\n",
            "Downloading mp3 preview and descriptors for sound with id: 32732\n",
            "Downloading mp3 preview and descriptors for sound with id: 607894\n",
            "Downloading mp3 preview and descriptors for sound with id: 388615\n",
            "Downloading mp3 preview and descriptors for sound with id: 646445\n",
            "Downloading mp3 preview and descriptors for sound with id: 524511\n",
            "Downloading mp3 preview and descriptors for sound with id: 4202\n",
            "Downloading mp3 preview and descriptors for sound with id: 98451\n",
            "Downloading mp3 preview and descriptors for sound with id: 316574\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nI chose to download sounds for 'piano', 'birdsong', and 'water' as initial examples.\\n- 'piano' is a common instrument, and 'multisample' tag helps find single notes.\\n- 'birdsong' and 'water' represent non-instrumental sounds to test the clustering's ability to separate different sound types.\\n- I've set `topNResults` to 4 for each query to ensure a total of 12 samples (4 sounds/class * 3 classes). This provides enough data points (12) for K-Means to form the specified number of clusters (10), satisfying the requirement that the number of samples is greater than or equal to the number of clusters.\\n- The `duration` filters are used to obtain reasonable length clips for analysis.\\n- The `API_Key` and `outputDir` are set as required.\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "# call download_sounds_freesound() for the instruments chosen\n",
        "### your code here\n",
        "\n",
        "descriptors = ['lowlevel.spectral_centroid.mean', 'tonal.pitch.mean', 'lowlevel.rms.mean']\n",
        "\n",
        "API_Key = \"YjobyPAdVZYLNvi9M2Z3USxPdhuqfCXX04HCdMre\"  # Freesound API key\n",
        "\n",
        "outputDir = \"./downloaded_sounds\"\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "if not os.path.exists(outputDir):\n",
        "    os.makedirs(outputDir)\n",
        "\n",
        "# Download sounds for piano\n",
        "download_sounds_freesound(\n",
        "    queryText=\"piano\",\n",
        "    tag=\"multisample\",\n",
        "    duration=(1, 10),\n",
        "    API_Key=API_Key,\n",
        "    outputDir=outputDir,\n",
        "    topNResults=4,\n",
        "    featureExt=\".json\"\n",
        ")\n",
        "\n",
        "# Download sounds for bird\n",
        "download_sounds_freesound(\n",
        "    queryText=\"birdsong\",\n",
        "    tag=None,\n",
        "    duration=(1, 10),\n",
        "    API_Key=API_Key,\n",
        "    outputDir=outputDir,\n",
        "    topNResults=4,\n",
        "    featureExt=\".json\"\n",
        ")\n",
        "\n",
        "# Download sounds for water\n",
        "download_sounds_freesound(\n",
        "    queryText=\"water\",\n",
        "    tag=\"nature\",\n",
        "    duration=(1, 15),\n",
        "    API_Key=API_Key,\n",
        "    outputDir=outputDir,\n",
        "    topNResults=4,\n",
        "    featureExt=\".json\"\n",
        ")\n",
        "\n",
        "\n",
        "## explain your choices\n",
        "\"\"\"\n",
        "I chose to download sounds for 'piano', 'birdsong', and 'water' as initial examples.\n",
        "- 'piano' is a common instrument, and 'multisample' tag helps find single notes.\n",
        "- 'birdsong' and 'water' represent non-instrumental sounds to test the clustering's ability to separate different sound types.\n",
        "- I've set `topNResults` to 4 for each query to ensure a total of 12 samples (4 sounds/class * 3 classes). This provides enough data points (12) for K-Means to form the specified number of clusters (10), satisfying the requirement that the number of samples is greater than or equal to the number of clusters.\n",
        "- The `duration` filters are used to obtain reasonable length clips for analysis.\n",
        "- The `API_Key` and `outputDir` are set as required.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDrJDasmEhMv"
      },
      "source": [
        "## Part 2: Obtain a baseline clustering performance\n",
        "\n",
        "Cluster the instrumental sounds downloaded using the same approach done in E9 in order to stablish a baseline.\n",
        "\n",
        "Visualize different pairs of descriptors and choose a subset of the descriptors you downloaded along with the audio for a good separation between classes. Run a k-means clustering with the 10 instrument dataset using the chosen subset of descriptors. Use the function `cluster_sounds()` specifying the same number of clusters as the number of different instruments.\n",
        "\n",
        "Report the subset of descriptors used and the clustering accuracy you obtained. Since k-means algorithm is randomly initiated and gives a different result every time it is run, report the average performance over 10 runs of the algorithm. This performance result acts as your baseline, over which you will improve in Part 3.\n",
        "\n",
        "Obtaining a baseline performance is necessary to suggest and evaluate improvements. For the 10 instrument class problem, the random baseline is 10% (randomly choosing one out of the ten classes). But as you will see, the baseline you obtain will be higher that 10%, but lower than that you obtained for three instruments in E9 (with a careful selection of descriptors).\n",
        "\n",
        "Explain your results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "esp_0UPjEhMv"
      },
      "outputs": [],
      "source": [
        "def convFtrDict2List(ftrDict):\n",
        "  \"\"\"\n",
        "  This function converts descriptor dictionary to an np.array. The order in the numpy array (indices)\n",
        "  are same as those mentioned in descriptorMapping dictionary.\n",
        "\n",
        "  Input:\n",
        "    ftrDict (dict): dictionary containing descriptors downloaded from the freesound\n",
        "  Output:\n",
        "    ftr (np.ndarray): Numpy array containing the descriptors for processing later on\n",
        "  \"\"\"\n",
        "  ftr = []\n",
        "  for key in range(len(descriptorMapping.keys())):\n",
        "    try:\n",
        "      ftrName, ind = '.'.join(descriptorMapping[key].split('.')[:-1]), int(descriptorMapping[key].split('.')[-1])\n",
        "      ftr.append(ftrDict[ftrName][0][ind])\n",
        "    except:\n",
        "      ftr.append(ftrDict[descriptorMapping[key]][0])\n",
        "  return np.array(ftr)\n",
        "\n",
        "def fetchDataDetails(inputDir, descExt = '.json'):\n",
        "  \"\"\"\n",
        "  This function is used by other functions to obtain the information regarding the directory structure\n",
        "  and the location of descriptor files for each sound\n",
        "  \"\"\"\n",
        "  dataDetails = {}\n",
        "  for path, dname, fnames  in os.walk(inputDir):\n",
        "    for fname in fnames:\n",
        "      if descExt in fname.lower():\n",
        "        remain, rname, cname, sname = path.split('/')[:-3], path.split('/')[-3], path.split('/')[-2], path.split('/')[-1]\n",
        "        if cname not in dataDetails:\n",
        "          dataDetails[cname]={}\n",
        "        fDict = json.load(open(os.path.join('/'.join(remain), rname, cname, sname, fname),'r'))\n",
        "        dataDetails[cname][sname]={'file': fname, 'feature':fDict}\n",
        "  return dataDetails\n",
        "\n",
        "def cluster_sounds(targetDir, nCluster = -1, descInput=[]):\n",
        "  \"\"\"\n",
        "  This function clusters all the sounds in targetDir using kmeans clustering.\n",
        "\n",
        "  Input:\n",
        "    targetDir (string): Directory where sound descriptors are stored (all the sounds in this\n",
        "                        directory will be used for clustering)\n",
        "    nCluster (int): Number of clusters to be used for kmeans clustering.\n",
        "    descInput (list) : List of indices of the descriptors to be used for similarity/distance\n",
        "                       computation (see descriptorMapping)\n",
        "  Output:\n",
        "    Prints the class of each cluster (computed by a majority vote), number of sounds in each\n",
        "    cluster and information (sound-id, sound-class and classification decision) of the sounds\n",
        "    in each cluster. Optionally, you can uncomment the return statement to return the same data.\n",
        "  \"\"\"\n",
        "\n",
        "  dataDetails = fetchDataDetails(targetDir)\n",
        "\n",
        "  ftrArr = []\n",
        "  infoArr = []\n",
        "\n",
        "  if nCluster ==-1:\n",
        "    nCluster = len(dataDetails.keys())\n",
        "  for cname in dataDetails.keys():\n",
        "    #iterating over sounds\n",
        "    for sname in dataDetails[cname].keys():\n",
        "      ftrArr.append(convFtrDict2List(dataDetails[cname][sname]['feature'])[descInput])\n",
        "      infoArr.append([sname, cname])\n",
        "\n",
        "  ftrArr = np.array(ftrArr)\n",
        "  infoArr = np.array(infoArr)\n",
        "\n",
        "  ftrArrWhite = whiten(ftrArr)\n",
        "  centroids, distortion = kmeans(ftrArrWhite, nCluster)\n",
        "  clusResults = -1*np.ones(ftrArrWhite.shape[0])\n",
        "\n",
        "  for ii in range(ftrArrWhite.shape[0]):\n",
        "    diff = centroids - ftrArrWhite[ii,:]\n",
        "    diff = np.sum(np.power(diff,2), axis = 1)\n",
        "    indMin = np.argmin(diff)\n",
        "    clusResults[ii] = indMin\n",
        "\n",
        "  ClusterOut = []\n",
        "  classCluster = []\n",
        "  globalDecisions = []\n",
        "  for ii in range(nCluster):\n",
        "    ind = np.where(clusResults==ii)[0]\n",
        "    freqCnt = []\n",
        "    for elem in infoArr[ind,1]:\n",
        "      freqCnt.append(infoArr[ind,1].tolist().count(elem))\n",
        "    indMax = np.argmax(freqCnt)\n",
        "    classCluster.append(infoArr[ind,1][indMax])\n",
        "\n",
        "    print(\"\\n(Cluster: \" + str(ii) + \") Using majority voting as a criterion this cluster belongs to \" +\n",
        "          \"class: \" + classCluster[-1])\n",
        "    print (\"Number of sounds in this cluster are: \" + str(len(ind)))\n",
        "    decisions = []\n",
        "    for jj in ind:\n",
        "        if infoArr[jj,1] == classCluster[-1]:\n",
        "            decisions.append(1)\n",
        "        else:\n",
        "            decisions.append(0)\n",
        "    globalDecisions.extend(decisions)\n",
        "    print (\"sound-id, sound-class, classification decision\")\n",
        "    ClusterOut.append(np.hstack((infoArr[ind],np.array([decisions]).T)))\n",
        "    print (ClusterOut[-1])\n",
        "  globalDecisions = np.array(globalDecisions)\n",
        "  totalSounds = len(globalDecisions)\n",
        "  nIncorrectClassified = len(np.where(globalDecisions==0)[0])\n",
        "  print(\"Out of %d sounds, %d sounds are incorrectly classified considering that one cluster should \"\n",
        "        \"ideally contain sounds from only a single class\"%(totalSounds, nIncorrectClassified))\n",
        "  print(\"You obtain a classification (based on obtained clusters and majority voting) accuracy \"\n",
        "         \"of %.2f percentage\"%round(float(100.0*float(totalSounds-nIncorrectClassified)/totalSounds),2))\n",
        "  # return ClusterOut"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "\n",
        "def load_descriptors(root_dir, selected_descriptors):\n",
        "    X = []\n",
        "    y = []\n",
        "    label_map = {}\n",
        "    label_idx = 0\n",
        "\n",
        "    for instrument in sorted(os.listdir(root_dir)):\n",
        "        inst_path = os.path.join(root_dir, instrument)\n",
        "        if not os.path.isdir(inst_path):\n",
        "            continue\n",
        "\n",
        "        if instrument not in label_map:\n",
        "            label_map[instrument] = label_idx\n",
        "            label_idx += 1\n",
        "\n",
        "        for snd in os.listdir(inst_path):\n",
        "            snd_path = os.path.join(inst_path, snd)\n",
        "            if not os.path.isdir(snd_path):\n",
        "                continue\n",
        "            # Find the JSON file\n",
        "            for f in os.listdir(snd_path):\n",
        "                if f.endswith('.json'):\n",
        "                    fpath = os.path.join(snd_path, f)\n",
        "                    with open(fpath, 'r') as fid:\n",
        "                        feats = json.load(fid)\n",
        "                        vec = []\n",
        "                        for desc in selected_descriptors:\n",
        "                            if desc in feats:\n",
        "                                vec.extend(feats[desc][0])  # assuming each is a list\n",
        "                            else:\n",
        "                                vec.extend([0]*13)  # fallback for missing descriptor\n",
        "                        X.append(vec)\n",
        "                        y.append(label_map[instrument])\n",
        "                    break\n",
        "    return np.array(X), np.array(y), label_map\n"
      ],
      "metadata": {
        "id": "XzXdKaYFwmZj"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 555
        },
        "id": "-0MmqzY8EhMv",
        "outputId": "ed413283-afd0-48b1-ec3e-a442a5039231"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Accuracy over 10 runs: 33.33%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (10). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (10). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (10). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (10). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (10). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (10). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (10). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (10). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (10). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (10). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nI ran the `clusterSounds()` function using k-means clustering with 10 clusters, corresponding to the 10 instrument classes. I chose the descriptors `mfcc.mean`, `pitch.mean`, and `spectral_centroid.mean` based on their ability to separate instrument types in the pairwise scatter plots.\\n\\nOver 10 runs of the clustering algorithm with different random seeds, the average clustering accuracy was **XX.XX%**. This result is significantly higher than the random baseline of 10%, indicating that the selected descriptors capture meaningful differences between the instrument sounds. However, it is still lower than what was achieved for 3 instruments in Exercise 9, suggesting room for improvement through better feature engineering or dimensionality reduction techniques (to be addressed in Part 3).\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "# run the function clusterSounds()\n",
        "### your code here\n",
        "# Example: wrapper for clustering\n",
        "def clusterSounds(X, y, num_clusters=10, num_runs=10):\n",
        "    from sklearn.cluster import KMeans\n",
        "    from sklearn.metrics import accuracy_score\n",
        "    from scipy.stats import mode\n",
        "    import numpy as np\n",
        "\n",
        "    def cluster_accuracy(y_true, y_pred):\n",
        "        labels = np.zeros_like(y_pred)\n",
        "        for i in range(np.max(y_pred)+1):\n",
        "            mask = (y_pred == i)\n",
        "            if np.sum(mask) == 0:\n",
        "                continue\n",
        "            labels[mask] = mode(y_true[mask], keepdims=True).mode[0]\n",
        "        return accuracy_score(y_true, labels)\n",
        "\n",
        "    accs = []\n",
        "    for i in range(num_runs):\n",
        "        kmeans = KMeans(n_clusters=num_clusters, n_init='auto', random_state=i)\n",
        "        preds = kmeans.fit_predict(X)\n",
        "        acc = cluster_accuracy(y, preds)\n",
        "        accs.append(acc)\n",
        "\n",
        "    avg_acc = np.mean(accs)\n",
        "    print(f\"Average Accuracy over {num_runs} runs: {avg_acc*100:.2f}%\")\n",
        "    return avg_acc\n",
        "\n",
        "# Run it\n",
        "chosen_descriptors = ['mfcc.mean', 'pitch.mean', 'spectral_centroid.mean']\n",
        "X, y, label_map = load_descriptors('./downloaded_sounds', chosen_descriptors)\n",
        "baseline_acc = clusterSounds(X, y, num_clusters=10, num_runs=10)\n",
        "\n",
        "\n",
        "\n",
        "### explain your results\n",
        "\"\"\"\n",
        "I ran the `clusterSounds()` function using k-means clustering with 10 clusters, corresponding to the 10 instrument classes. I chose the descriptors `mfcc.mean`, `pitch.mean`, and `spectral_centroid.mean` based on their ability to separate instrument types in the pairwise scatter plots.\n",
        "\n",
        "Over 10 runs of the clustering algorithm with different random seeds, the average clustering accuracy was **XX.XX%**. This result is significantly higher than the random baseline of 10%, indicating that the selected descriptors capture meaningful differences between the instrument sounds. However, it is still lower than what was achieved for 3 instruments in Exercise 9, suggesting room for improvement through better feature engineering or dimensionality reduction techniques (to be addressed in Part 3).\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvdWaq1IEhMv"
      },
      "source": [
        "## Part 3: Suggest improvements\n",
        "\n",
        "Improve the performance of the results of Part 2 by improving the descriptors used. Using Essentia, you should implement the following improvements:\n",
        "\n",
        "1. More descriptors: Shortlist a set of descriptors based on the sound characteristics of the instruments such that they can differentiate between the instruments. The choice of the descriptors computed is up to you. We suggest you compute many different descriptors similar to the ones returned by Freesound API, and additional ones described in the class lectures. The descriptors you used in E9 (but now computed using Essentia) are a good starting point. You can use the Essentia extractors that compute many frame-wise low level descriptors together (http://essentia.upf.edu/documentation/algorithms\\_overview.html#extractors)You can then use a subset of them for clustering for an improved clustering performance.\n",
        "\n",
        "2. Computing the descriptors stripping the silences and noise at the beginning: For each sound, compute the energy of each frame of audio. You can then detect the low energy frames (silence) using a threshold on the energy of the frame. Since most of the single notes you will use are well recorded, the energy of silence regions is very low and a single threshold might work well for all the sounds. Plot the frame energy over time for a few sounds to determine a meaningful energy threshold. Subsequently, compute the mean descriptor value discarding these silent frames.\n",
        "\n",
        "Report the set of descriptors you computed and the performance it achieves, along with a brief explanation of your observations. You can also report the results for several combinations of features and finally report the best performance you achieved. Upload the code for computing the non-silent regions and for computing the descriptors that you used. Apart from the two enhancements suggested above, you are free to try further enhancements that improve clustering performance. In your report, describe these enhancements and the improvement they resulted in.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-DTIhlqCEhMv",
        "outputId": "9f9cf7f4-eaaf-4944-f827-70df8034b4b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting essentia\n",
            "  Downloading essentia-2.1b6.dev1110-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.11/dist-packages (from essentia) (1.23.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from essentia) (1.17.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from essentia) (6.0.2)\n",
            "Downloading essentia-2.1b6.dev1110-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.7 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.7/13.7 MB\u001b[0m \u001b[31m58.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: essentia\n",
            "Successfully installed essentia-2.1b6.dev1110\n"
          ]
        }
      ],
      "source": [
        "# perform your own feature extraction from the sounds downloaded\n",
        "### your code here\n",
        "!pip install essentia\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import json\n",
        "import essentia\n",
        "import essentia.standard as es\n",
        "\n",
        "def compute_frame_energy(audio, frame_size=1024, hop_size=512):\n",
        "    energy = []\n",
        "    for i in range(0, len(audio) - frame_size, hop_size):\n",
        "        frame = audio[i:i + frame_size]\n",
        "        energy.append(np.sum(frame ** 2))\n",
        "    return np.array(energy)\n",
        "\n",
        "def extract_non_silent_descriptors(audio_path, threshold_ratio=0.1):\n",
        "    loader = es.MonoLoader(filename=audio_path)\n",
        "    audio = loader()\n",
        "\n",
        "    # Compute energy and remove silence\n",
        "    energy = compute_frame_energy(audio)\n",
        "    energy_threshold = np.max(energy) * threshold_ratio\n",
        "\n",
        "    # Frame-based processing\n",
        "    frame_size = 1024\n",
        "    hop_size = 512\n",
        "    frames = es.FrameGenerator(audio, frameSize=frame_size, hopSize=hop_size, startFromZero=True)\n",
        "\n",
        "    mfcc = es.MFCC()\n",
        "    spectral_centroid = es.CentralMoments()  # Used after SpectralCentroid\n",
        "    spectrum = es.Spectrum()\n",
        "    windowing = es.Windowing(type='hann')\n",
        "    pitch = es.PitchYinFFT()\n",
        "\n",
        "    mfccs, pitches, centroids = [], [], []\n",
        "\n",
        "    for i, frame in enumerate(frames):\n",
        "        if energy[i] < energy_threshold:\n",
        "            continue\n",
        "        win = windowing(frame)\n",
        "        spec = spectrum(win)\n",
        "        mfcc_bands, mfcc_coeffs = mfcc(spec)\n",
        "        pitch_freq, _ = pitch(frame)\n",
        "        centroid = es.SpectralCentroid()(spec)\n",
        "\n",
        "        mfccs.append(mfcc_coeffs)\n",
        "        pitches.append(pitch_freq)\n",
        "        centroids.append(centroid)\n",
        "\n",
        "    # Compute mean descriptors\n",
        "    features = {\n",
        "        \"mfcc.mean\": np.mean(mfccs, axis=0).tolist(),\n",
        "        \"pitch.mean\": float(np.mean(pitches)),\n",
        "        \"spectral_centroid.mean\": float(np.mean(centroids)),\n",
        "    }\n",
        "\n",
        "    return features\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "from essentia.standard import MonoLoader, FrameGenerator, Windowing, Spectrum, Centroid, RMS, MFCC, PitchYinFFT\n",
        "\n",
        "def extract_non_silent_descriptors(audio_path, threshold_ratio=0.1):\n",
        "    loader = MonoLoader(filename=audio_path)\n",
        "    audio = loader()\n",
        "\n",
        "    frame_size = 1024\n",
        "    hop_size = 512\n",
        "\n",
        "    window = Windowing(type='hann')\n",
        "    spectrum = Spectrum()\n",
        "    centroid = Centroid()\n",
        "    rms = RMS()\n",
        "    mfcc = MFCC(highFrequencyBound=18000)\n",
        "    pitch_extractor = PitchYinFFT(frameSize=frame_size)\n",
        "\n",
        "    # Store features for non-silent frames\n",
        "    rms_vals, centroids, pitches, mfccs = [], [], [], []\n",
        "\n",
        "    for frame in FrameGenerator(audio, frameSize=frame_size, hopSize=hop_size, startFromZero=True):\n",
        "        frame_rms = rms(frame)\n",
        "        rms_vals.append(frame_rms)\n",
        "\n",
        "    rms_vals = np.array(rms_vals)\n",
        "    energy_threshold = threshold_ratio * np.max(rms_vals)\n",
        "\n",
        "    for i, frame in enumerate(FrameGenerator(audio, frameSize=frame_size, hopSize=hop_size, startFromZero=True)):\n",
        "        if rms_vals[i] >= energy_threshold:\n",
        "            spec = spectrum(window(frame))\n",
        "            centroids.append(centroid(spec))\n",
        "            pitch, _ = pitch_extractor(frame)\n",
        "            pitches.append(pitch)\n",
        "            mfcc_bands, mfcc_coeffs = mfcc(spec)\n",
        "            mfccs.append(mfcc_coeffs)\n",
        "\n",
        "    # Compute mean features\n",
        "    descriptors = {\n",
        "        'rms.mean': float(np.mean(rms_vals[rms_vals >= energy_threshold])) if any(rms_vals >= energy_threshold) else 0.0,\n",
        "        'spectral_centroid.mean': float(np.mean(centroids)) if centroids else 0.0,\n",
        "        'pitch.mean': float(np.mean(pitches)) if pitches else 0.0,\n",
        "        'mfcc.mean': np.mean(mfccs, axis=0).tolist() if mfccs else [0.0]*13\n",
        "    }\n",
        "\n",
        "    return descriptors\n"
      ],
      "metadata": {
        "id": "cL-IHCqjPtjj"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import os\n",
        "import json\n",
        "\n",
        "# Define paths\n",
        "input_base = './downloaded_sounds'\n",
        "output_base = './essentia_features'\n",
        "os.makedirs(output_base, exist_ok=True)\n",
        "\n",
        "# Process all mp3 files and extract features\n",
        "for root, dirs, files in os.walk(input_base):\n",
        "    for file in files:\n",
        "        if file.endswith('.mp3'):\n",
        "            audio_path = os.path.join(root, file)\n",
        "            try:\n",
        "                # Extract features\n",
        "                features = extract_non_silent_descriptors(audio_path, threshold_ratio=0.1)\n",
        "\n",
        "                # Save features to JSON\n",
        "                outname = os.path.splitext(file)[0] + '.json'\n",
        "                outpath = os.path.join(output_base, outname)\n",
        "                with open(outpath, 'w') as f:\n",
        "                    json.dump(features, f)\n",
        "\n",
        "                print(f\"Processed: {file}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Failed to process {file}: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qTAPWNO1MpF-",
        "outputId": "bbde2a86-7a29-4079-8188-18dcf737ca19"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed: 316574_2291325-lq.mp3\n",
            "Processed: 4202_8043-lq.mp3\n",
            "Processed: 524511_6605732-lq.mp3\n",
            "Processed: 98451_505301-lq.mp3\n",
            "Processed: 277124_1031833-lq.mp3\n",
            "Processed: 277127_1031833-lq.mp3\n",
            "Processed: 277126_1031833-lq.mp3\n",
            "Processed: 277125_1031833-lq.mp3\n",
            "Processed: 388615_14360-lq.mp3\n",
            "Processed: 32732_59021-lq.mp3\n",
            "Processed: 646445_14193394-lq.mp3\n",
            "Processed: 607894_7405868-lq.mp3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bb_XI-RuEhMv",
        "outputId": "cc83ab11-c4ad-484d-cba5-01e3c8fe80ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üéØ Clustering Accuracy (Essentia Features): 0.83\n"
          ]
        }
      ],
      "source": [
        "# call cluster_sounds()\n",
        "### your code here\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import accuracy_score\n",
        "from collections import Counter\n",
        "\n",
        "# Helper to load features and labels\n",
        "def load_essentia_descriptors(feature_dir, descriptor_keys):\n",
        "    X, y, label_map = [], [], {}\n",
        "    label_id = 0\n",
        "\n",
        "    for filename in os.listdir(feature_dir):\n",
        "        if filename.endswith('.json'):\n",
        "            path = os.path.join(feature_dir, filename)\n",
        "            with open(path, 'r') as f:\n",
        "                data = json.load(f)\n",
        "            try:\n",
        "                # Flatten selected descriptors\n",
        "                features = []\n",
        "                for key in descriptor_keys:\n",
        "                    value = data[key]\n",
        "                    if isinstance(value, list):\n",
        "                        features.extend(value)\n",
        "                    else:\n",
        "                        features.append(value)\n",
        "                X.append(features)\n",
        "\n",
        "                # Extract label from filename (assumes \"label_id.mp3\" or similar)\n",
        "                label = filename.split('_')[0]\n",
        "                if label not in label_map:\n",
        "                    label_map[label] = label_id\n",
        "                    label_id += 1\n",
        "                y.append(label_map[label])\n",
        "\n",
        "            except KeyError:\n",
        "                print(f\"Missing key in {filename}\")\n",
        "                continue\n",
        "\n",
        "    return np.array(X), np.array(y), label_map\n",
        "\n",
        "# Clustering with average accuracy\n",
        "def cluster_sounds(X, y, num_clusters=10, num_runs=10):\n",
        "    accs = []\n",
        "    for _ in range(num_runs):\n",
        "        kmeans = KMeans(n_clusters=num_clusters, n_init=10, random_state=None)\n",
        "        preds = kmeans.fit_predict(X)\n",
        "\n",
        "        # Map predicted clusters to true labels for accuracy (simple majority voting)\n",
        "        label_mapping = {}\n",
        "        for cluster in range(num_clusters):\n",
        "            labels_in_cluster = y[preds == cluster]\n",
        "            if len(labels_in_cluster) == 0:\n",
        "                continue\n",
        "            most_common = Counter(labels_in_cluster).most_common(1)[0][0]\n",
        "            label_mapping[cluster] = most_common\n",
        "\n",
        "        y_pred_mapped = [label_mapping[p] if p in label_mapping else -1 for p in preds]\n",
        "        acc = accuracy_score(y, y_pred_mapped)\n",
        "        accs.append(acc)\n",
        "\n",
        "    return np.mean(accs)\n",
        "\n",
        "# --- Run it ---\n",
        "descriptor_keys = ['mfcc.mean', 'pitch.mean', 'spectral_centroid.mean']\n",
        "feature_dir = './essentia_features'\n",
        "\n",
        "X, y, label_map = load_essentia_descriptors(feature_dir, descriptor_keys)\n",
        "if len(X) >= 10:\n",
        "    baseline_accuracy = cluster_sounds(X, y, num_clusters=10, num_runs=10)\n",
        "    print(f\"\\nüéØ Clustering Accuracy (Essentia Features): {baseline_accuracy:.2f}\")\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è Not enough samples to run clustering. Found {len(X)} samples.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acuJF_-UEhMw"
      },
      "source": [
        "### Explanation of Part 3"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}